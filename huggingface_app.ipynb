{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import BertTokenizer, BertForMaskedLM, BertForNextSentencePrediction,BertForQuestionAnswering\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "model_name = 'dccuchile/bert-base-spanish-wwm-uncased'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Tokenizer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Utilizar el tokenizer de forma básico (separa el texto por palabra/sub-palabra),con la función tokenize se puede tokenizar."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "sentence = \"M47 Labs es una compañía dedicada al analisis de lenguaje, esta ubicada en Barcelona,Cataluña\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "tokenizer.tokenize(sentence)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['m',\n",
       " '##4',\n",
       " '##7',\n",
       " 'lab',\n",
       " '##s',\n",
       " 'es',\n",
       " 'una',\n",
       " 'compañía',\n",
       " 'dedicada',\n",
       " 'al',\n",
       " 'anal',\n",
       " '##isis',\n",
       " 'de',\n",
       " 'lenguaje',\n",
       " ',',\n",
       " 'esta',\n",
       " 'ubicada',\n",
       " 'en',\n",
       " 'barcelona',\n",
       " ',',\n",
       " 'cataluña']"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Condificar el input para alimentar a un modelo"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Este proceso consiste en 3 pasos primero separar el texto en palabras,sub-palabras o letras, luego se agregan los tokens especiales [cls],[sep] y por ultimo se le asigna un valor a cada token dependiendo del vocabulario"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "tokenizer.encode_plus(sentence,padding=\"max_length\", max_length=50)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': [4, 1023, 1003, 998, 5004, 30958, 1028, 1091, 3663, 10639, 1074, 6634, 3838, 1009, 8023, 1019, 1149, 6616, 1035, 5779, 1019, 10373, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "inouts_ids : valor numerico del token asignado por el vocabulario.\n",
    "\n",
    "attention_mask: vector booleano que indica si se le presta atencion a dichos tokens, en caso de tener padding asigna el valor cero al token especial [PAD]\n",
    "\n",
    "token_type_ids: para pares de oraciones (oracion1,oracion2) indica a que oracion pertenece un token. (1,0)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Otra aplicacion es convertir los tokens_ids a palabras, este proceso se llama decod"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "decode_sentence = tokenizer.decode([4, 1023, 1003, 998, 5004, 30958, 1028, 1091, 3663, 10639, 1074, 6634, 3838, 1009, 8023, 1019, 1149, 6616, 1035, 5779, 1019, 10373, 5])\n",
    "\n",
    "decode_sentence"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'[CLS] m47 labs es una compañía dedicada al analisis de lenguaje, esta ubicada en barcelona, cataluña [SEP]'"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Aplicaciones Basicas de HF"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Mask Model: es la tarea de decodificar un token enmascarado(incognito) , rellenar los espacion en blanco."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "tokenizer_mask = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model_mask = BertForMaskedLM.from_pretrained(model_name, return_dict = True)\n",
    "\n",
    "texto  = \"La Torre Eiffel esta ubicada en \" + tokenizer.mask_token + \", Francia.\"\n",
    "\n",
    "# input del modelo contiene input_ids,attention_maks,token_type_ids\n",
    "input = tokenizer.encode_plus(texto, return_tensors = \"pt\")\n",
    "\n",
    "# ubico mi elemneto a estimar\n",
    "mask_index = torch.where(input[\"input_ids\"][0] == tokenizer.mask_token_id)\n",
    "\n",
    "\n",
    "# calculasmo los logits \n",
    "output = model_mask(**input)\n",
    "\n",
    "\n",
    "logits = output.logits\n",
    "\n",
    "#aplicamos la funcion softmax en la dimension columnas\n",
    "softmax = F.softmax(logits, dim = -1)\n",
    "\n",
    "# tomamos la matriz correspondiente a el elemento cubierto\n",
    "mask_word = softmax[0, mask_index, :]\n",
    "\n",
    "# seleccionamos los 10 token_ids con meyor ranking\n",
    "top_10 = torch.topk(mask_word, 10, dim = 1)[1][0]\n",
    "\n",
    "for token in top_10:\n",
    "\n",
    "   word = tokenizer.decode([token])\n",
    "\n",
    "   new_sentence = texto.replace(tokenizer.mask_token, word)\n",
    "\n",
    "   print(new_sentence)\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "La Torre Eiffel esta ubicada en paris, Francia.\n",
      "La Torre Eiffel esta ubicada en francia, Francia.\n",
      "La Torre Eiffel esta ubicada en marsella, Francia.\n",
      "La Torre Eiffel esta ubicada en cannes, Francia.\n",
      "La Torre Eiffel esta ubicada en france, Francia.\n",
      "La Torre Eiffel esta ubicada en ginebra, Francia.\n",
      "La Torre Eiffel esta ubicada en estrasburgo, Francia.\n",
      "La Torre Eiffel esta ubicada en lyon, Francia.\n",
      "La Torre Eiffel esta ubicada en niza, Francia.\n",
      "La Torre Eiffel esta ubicada en [UNK], Francia.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Next Sentence Prediction: se busca si una oracion sigue a otra oracion\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model_nsp = BertForNextSentencePrediction.from_pretrained(model_name)\n",
    "sentence_1 = \"El niño vino a casa de la escuela\"\n",
    "sentence_2 = \"Luego de la escuela,fue a su clase de natación .\"\n",
    "input = bert_tokenizer.encode_plus(sentence_1,sentence_2,return_tensors='pt')\n",
    "outputs = model_nsp(**input)[0]\n",
    "softmax = F.softmax(outputs, dim = 1)\n",
    "print(softmax)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForNextSentencePrediction: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForNextSentencePrediction were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['bert.pooler.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.4168, 0.5832]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Zero Shot Classification: asigna una etiqueta a un texto, el input es un texto y un serie de etiquetas "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "model_zs = \"facebook/bart-large-mnli\"\n",
    "zs_classifier = pipeline(\"zero-shot-classification\",\n",
    "                      model =  model_zs)\n",
    "\n",
    "\n",
    "sequence_to_classify = \"El partido estuvo muy parajo, el primer gol fue muy bueno\"\n",
    "candidate_labels = ['deportes', 'politica', 'economia']\n",
    "zs_classifier(sequence_to_classify, candidate_labels)\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'sequence': 'El partido estuvo muy parajo, el primer gol fue muy bueno',\n",
       " 'labels': ['politica', 'economia', 'deportes'],\n",
       " 'scores': [0.45973241329193115, 0.2714166045188904, 0.2688509225845337]}"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Question Answering: el modelo reponde una pregunta basandose en un contexto"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "\n",
    "model_name_qs = 'mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es'\n",
    "model_qa = BertForQuestionAnswering.from_pretrained(model_name_qs)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name_qs)\n",
    "\n",
    "pregunta = 'Donde trabaj enrique?'\n",
    "\n",
    "texto = \"Enrique es ingeniero mecanico y trabaja en Mclaren\"\n",
    "\n",
    "# se debe alimentar el  tokenizer con la pregunta  y el texto\n",
    "encoding = tokenizer.encode_plus(pregunta, texto,return_tensors='pt')\n",
    "tokens = tokenizer.convert_ids_to_tokens(encoding['input_ids'][0])\n",
    "\n",
    "# se alimenta el modelo con el texto codificado\n",
    "output = model_qa(**encoding)\n",
    "\n",
    "\n",
    "\n",
    "answer_start = torch.argmax(output.start_logits)\n",
    "\n",
    "answer_end = torch.argmax(output.end_logits)\n",
    "\n",
    "if answer_end >= answer_start:\n",
    "     answer = tokens[answer_start]\n",
    "     for i in range(answer_start+1, answer_end+1):\n",
    "         if tokens[i][0:2] == \"##\":\n",
    "             answer += tokens[i][2:]\n",
    "         else:\n",
    "             answer += \" \" + tokens[i]\n",
    "            \n",
    "if answer.startswith(\"[CLS]\"):\n",
    "     answer = \"Unable to find the answer to your question.\"\n",
    "\n",
    "print(\"\\nPredicted answer:\\n{}\".format(answer.capitalize()))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Predicted answer:\n",
      "Mclaren\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('q_a': virtualenv)"
  },
  "interpreter": {
   "hash": "a6756d8d4a2ccb3ab7a424c8319f10ce277a1a98cac6be489aabed8f525e5ca8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}